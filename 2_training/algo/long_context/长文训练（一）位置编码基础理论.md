# 位置编码理论基础

位置编码是为了让两个token之间的相对位置关系能够被模型感知。工业界大模型基本都采用RoPE（Rotary Position Embedding）位置编码，因此本文只讨论RoPE。

## Attention公式定义

Attention的计算涉及 
$q$ 
（query）、
$k$
（key）和
$v$
（value）三个向量，计算公式如下：

$$a_{m,n} = \frac{exp( \frac{q_m \cdot k_n}{\sqrt{d}})}{\sum_{j=1}^{N} exp(\frac{q_m \cdot k_j}{\sqrt{d}})} (1)$$

![img](https://github.com/monster119120/Industrial_LLM_tutorial/blob/main/2_training/algo/long_context/attention_formula.png)

## RoPE位置编码

实际上在计算公式1之前，已经对词向量
$q$
和
$k$
注入了位置信息，我们以
$q_m=[x_0, x_1, \ldots, x_{d-1}]$
为例，计算公式如下：

$$
q_m=
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3 \\
\vdots \\
x_{d-2} \\
x_{d-1}
\end{pmatrix}
\begin{pmatrix}
2
\end{pmatrix}
$$

其中
$$
\theta_i = 10000^{-2i/d}
$$

让我们回顾一下函数
$y = 10000^{-x}$
的图像如下，
$i$
越大，则
$\theta_i$
越小。**也就是说
$q_m$
中维度越高，转过的角度越少**。

![img](https://github.com/monster119120/Industrial_LLM_tutorial/blob/main/2_training/algo/long_context/theta_formula.png)


## RoPE编码的代码实现

由于上述公式(2)中的矩阵乘法计算很稀疏，因此可以转化为下述的公式(3)进行运算

$$
q_m =
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3 \\
\vdots \\
x_{d-2} \\
x_{d-1}
\end{pmatrix}
\otimes
\begin{pmatrix}
\cos m \theta_0 \\
\cos m \theta_0 \\
\cos m \theta_1 \\
\cos m \theta_1 \\
\vdots \\
\cos m \theta_{d/2-1} \\
\cos m \theta_{d/2-1}
\end{pmatrix}
+
\begin{pmatrix}
-x_1 \\
x_0 \\
-x_3 \\
x_2 \\
\vdots \\
-x_{d-1} \\
x_{d-2}
\end{pmatrix}
\otimes
\begin{pmatrix}
\sin m \theta_0 \\
\sin m \theta_0 \\
\sin m \theta_1 \\
\sin m \theta_1 \\
\vdots \\
\sin m \theta_{d/2-1} \\
\sin m \theta_{d/2-1}
\end{pmatrix}
\begin{pmatrix}
3
\end{pmatrix}
$$


接下来，我们分析transformer官方早期对RoPE的实现[RoPE代码实现](https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py)：

```python
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]
    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])
    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

代码实现的RoPE与我们的公式(3)虽然不一样，但其实是完全等价的。首先我们可以把
$q_m$
按照偶数在前，奇数在后重新排布一下，如公式(4)：

$$
q_m =
\begin{pmatrix}
x_0 \\
x_2 \\
x_4 \\
\vdots \\
x_{1} \\
x_{3} \\
x_{5} \\
\vdots
\end{pmatrix}
\otimes
\begin{pmatrix}
\cos m \theta_0 \\
\cos m \theta_1 \\
\cos m \theta_2 \\
\vdots \\
\cos m \theta_0 \\
\cos m \theta_1 \\
\cos m \theta_2 \\
\vdots 
\end{pmatrix}
+
\begin{pmatrix}
-x_1 \\
-x_3 \\
-x_5 \\
\vdots \\
x_{2} \\
x_{4} \\
x_{6} \\
\vdots \\
\end{pmatrix}
\otimes
\begin{pmatrix}
\sin m \theta_0 \\
\sin m \theta_1 \\
\sin m \theta_2 \\
\vdots \\
\sin m \theta_0 \\
\sin m \theta_1 \\
\sin m \theta_2 \\
\vdots 
\end{pmatrix}
\begin{pmatrix}
4
\end{pmatrix}
$$