# 长度外推

大模型预训练的文本通常较短（比如4K），因为直接在长文本上（比如128K）预训练模型的成本非常大。
为了让模型能够处理更长的文本，工业界一般先在短文本上预训练模型，然后再利用长度外推技术让大模型有处理长文本的能力，从而降低训练成本。

## RoPE理论

根据我们在 [长文训练（一）位置编码基础理论](https://zhuanlan.zhihu.com/p/1933621399240569735) 中的公式推导，计算
$$q_m$$
和
$$k_n$$
之间的点积，计算公式可以表示为如下：

$$
q_m^T \cdot k_n =
q_m^T
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}^T
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}
k_n
\begin{pmatrix}
1
\end{pmatrix}
$$

可以继续简化为

$$
q_m^T \cdot k_n =
q_m^T
\begin{pmatrix}
\cos ((m-n) \theta_0) & -\sin ((m-n) \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
\sin ((m-n) \theta_0) & \cos ((m-n) \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos ((m-n) \theta_1) & -\sin ((m-n) \theta_1) & \cdots & 0 & 0 \\
0 & 0 & \sin ((m-n) \theta_1) & \cos ((m-n) \theta_1) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos ((m-n) \theta_{d / 2-1}) & -\sin ((m-n) \theta_{d / 2-1})  \\
0 & 0 & 0 & 0 & \cdots & \sin ((m-n) \theta_{d / 2-1}) & \cos ((m-n) \theta_{d / 2-1})
\end{pmatrix}
k_n
\begin{pmatrix}
2
\end{pmatrix}
$$

因此我们可以看到，位置编码对Attention的影响只取决于两个token之间的相对位置
$$m-n$$
以及
$$\theta_i$$
的值。

## 长度外推

假设我们现在已经训好了一个大模型，预训练长度为
$$L$$
，则模型见过的两个token之间的相对位置
$$m-n$$
的最大值为
$$L-1$$
，最小值为0。

大模型在超过自身预训练长度
$$L$$
时，其旋转矩阵的值就会超出预训练时的范围，导致推理效果变差。


TODO:
NTK
YARN