# 长度外推

大模型预训练的文本通常较短（比如4K），因为直接在长文本上（比如128K）预训练模型的成本非常大。
为了让模型能够处理更长的文本，工业界一般先在短文本上预训练模型，然后再利用长度外推技术让大模型有处理长文本的能力，从而降低训练成本。

## RoPE理论

根据我们在 [长文训练（一）位置编码基础理论](https://zhuanlan.zhihu.com/p/1933621399240569735) 中的公式推导，计算
$$q_m$$
和
$$k_n$$
之间的点积，计算公式可以表示为如下：

$$
q_m^T \cdot k_n =
q_m^T
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}^T
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}
k_n
\begin{pmatrix}
1
\end{pmatrix}
$$

可以继续简化为

$$
q_m^T \cdot k_n =
q_m^T
\begin{pmatrix}
\cos ((m-n) \theta_0) & -\sin ((m-n) \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
\sin ((m-n) \theta_0) & \cos ((m-n) \theta_0) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos ((m-n) \theta_1) & -\sin ((m-n) \theta_1) & \cdots & 0 & 0 \\
0 & 0 & \sin ((m-n) \theta_1) & \cos ((m-n) \theta_1) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos ((m-n) \theta_{d / 2-1}) & -\sin ((m-n) \theta_{d / 2-1})  \\
0 & 0 & 0 & 0 & \cdots & \sin ((m-n) \theta_{d / 2-1}) & \cos ((m-n) \theta_{d / 2-1})
\end{pmatrix}
k_n
\begin{pmatrix}
2
\end{pmatrix}
$$

因此我们可以看到，位置编码对Attention的影响只取决于两个token之间的相对位置
$$m-n$$
以及
$$\theta_i$$
的值。

## 第一类长度外推方法：将推理位置“压缩”回训练范围内

大模型在预训练阶段，学习和记忆了在特定上下文长度（比如 $L_{train}=4096$）内的位置信息。如果推理时输入的文本长度超过了 $L_{train}$，模型就会遇到它从未“见过”的位置，导致其理解能力和生成效果急剧下降。

下面介绍的方法，其核心思想都是将超长的推理文本位置，通过某种方式“压缩”或“映射”回模型熟悉的 $0$ 到 $L_{train}-1$ 的范围之内。

### 位置插值法（Position Interpolation, PI）

位置插值法（PI）是实现这一目标的最直观、最经典的方法。

#### 核心思想

PI的核心思想非常简单：如果推理的文本长度 $L_{inference}$ 是预训练长度 $L_{train}$ 的 `N` 倍，那么我们就将每个词元（token）的位置索引（position index）都除以 `N`，从而将新的位置“插值”到旧的、更小的位置范围中。

#### 一个生动的例子

我们可以用一把尺子来打比方：

*   **原始模型**：好比一把只有 **4米** 长的尺子，它对0到4米内的所有刻度都了如指掌。
*   **长文本推理**：现在需要用它去测量一个 **8米** 的物体，尺子显然不够长了。
*   **PI的解决方案**：我们不改变尺子的物理长度，而是重新定义其刻度。具体来说，就是把所有刻度值都“压缩”一半。这样，原来 `8` 米处的位置，现在对应到了尺子上的 `4` 米刻度；`4` 米处的位置对应 `2` 米刻度。通过这种方式，这把4米长的尺子就能够测量8米的物体了。

回到模型上，当预训练长度为 `4`，而推理长度为 `8` 时，PI会将 `[0, 1, 2, 3, 4, 5, 6, 7]` 这些位置，通过除以2（即 `8/4`）的方式，映射为 `[0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5]`。这些新的位置编码全部落在了模型熟悉的 `[0, 3]` 范围之内，模型因此就能够理解它们了。

#### 技术实现

Position Interpolation的做法简单且高效。在技术上，它通过缩小RoPE（旋转位置编码）中的旋转角度来实现。简单来说，**要将文本长度扩展几倍，就将每个位置的旋转角度缩小几倍**。

最终，对于任意位置 $m$ 和目标推理上下文长度 $L_{inference}$，其在预训练长度 $L_{train}$ 下的等效位置 $m'$ 计算公式如下：

$$
m' = m \times \frac{L_{train}}{L_{inference}}
$$

这个新的位置 $m'$ 会被代入到原始的RoPE计算中，从而让模型能够理解超出预训练长度的文本。


### NTK-aware Position Interpolation

虽然PI方法很有效，但它有一个核心缺陷：**它对所有频率的旋转角度进行了无差别的等比例缩放**。

这会导致一个问题：模型在预训练中学到的高频信息（代表token之间的细微、局部关系）会被“模糊”或“稀释”。打个比方，就像为了把一张高清图片缩小，我们直接降低了它的整体分辨率，导致图片中的锐利细节丢失了。

#### 核心思想

为了解决这个问题，NTK-aware（神经正切核感知）插值法被提了出来。它的核心思想是：**不再对所有频率“一视同仁”，而是进行差异化缩放**。具体来说：

*   **对于高频分量**（负责捕捉邻近token间的关系）：少压缩，甚至不压缩，保留其原有的分辨率。
*   **对于低频分量**（负责捕捉远距离token间的关系）：多压缩，将其拉入训练范围内。

这样一来，模型既能保持对局部细节的精确感知，又能理解长距离的依赖关系。

#### 技术实现

NTK-aware插值法通过修改RoPE的 **基数（base）** 来实现这种差异化缩放。原始RoPE的基数 `b` 通常是10000。NTK方法会根据要扩展的长度倍数 `N`，计算出一个新的基数 `b'`：

$$
b' = b \times N^{(d / (d-2))}
$$

其中 `d` 是模型的隐藏维度。这个新的基数 `b'` 会使得高频信息的旋转角度变化较小，而低频信息变化较大，从而实现了高频“外推”（extrapolation）和低频“内插”（interpolation）的结合。

简单来说，NTK-aware方法在保持高频细节（不插值或少插值）的同时，将低频部分（长距离依赖）平滑地插入到模型的认知范围内，解决了PI方法中分辨率损失的问题。

### NTK-by-parts Interpolation

NTK-aware方法虽然有所改进，但它在某些维度上仍然进行了轻微的“外推”，这可能在微调时引入不稳定性。此外，即使是非线性压缩，也可能让模型对非常近的token顺序感到“困惑”。

NTK-by-parts（分部NTK）插值法是对NTK-aware的进一步精炼。它不再使用平滑的过渡，而是根据频率的波长 `λ` 和上下文长度 `L` 的关系，将频率维度清晰地划分为三个部分，并采取不同的策略：

1.  **高频部分 (λ << L)**：完全不进行插值。直接依赖模型原始的外推能力。
2.  **低频部分 (λ >= L)**：进行完全的插值，将其压缩回训练范围内。
3.  **中频部分**：采用类似于NTK-aware的混合策略。

这种方法更加精细地平衡了保留细节与扩展长度之间的关系，通过一个“斜坡函数”（ramp function）`γ` 来根据比率 `r = L/λ` 平滑地在不同策略间过渡，为不同频率的token提供了更合适的处理方式。

### YARN

YARN（Yet another RoPE extensioN method）可以说是集大成者，它在“NTK-by-parts”的基础上，引入了一个非常关键的额外组件：**注意力缩放（Attention Scaling）**。

#### 核心思想

YARN认为，仅仅修改位置编码还不够，还需要调整注意力分数的计算方式来进一步稳定模型。它将“NTK-by-parts”的精细化位置插值与一个**温度参数 `t`** 结合起来。

#### 关键创新

1.  **基于NTK-by-parts的插值**：YARN采用了NTK-by-parts的频率选择性插值策略来处理位置信息，确保了位置编码的准确性。
2.  **注意力温度缩放**：这是YARN的核心创新。在计算注意力权重时，它会在Softmax函数之前，用一个温度 `t` 来缩放注意力得分矩阵（即 `softmax(QK^T / t)`）。这个简单的缩放操作可以有效平滑注意力分布，防止出现极端的大值或小值，从而在整个扩展的上下文窗口中稳定模型的困惑度（perplexity）。这一步可以通过直接缩放 `q` 和 `k` 向量来实现，几乎不增加任何计算开销。
3.  **动态缩放（Dynamic Scaling）**：在推理时，YARN可以根据当前序列的实际长度动态调整缩放因子，而不是使用一个固定的扩展长度。这使得模型在处理不同长度的文本时更加灵活，即使超过了微调时设定的长度，性能也能平稳地下降，而不是突然崩溃。

#### 工业界应用

YARN被证明是一种极其高效的上下文扩展方法。它需要的微调数据和训练步数远少于之前的方法，并且成功地将Llama 2等模型的上下文窗口扩展到了**128k**，同时在标准基准测试中保持了原有的性能。实际上工业界的模型大多也采用YARN的外推方法。
