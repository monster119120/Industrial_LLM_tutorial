# 长文外扩

大模型预训练的文本通常较短（比如4K），因为直接在长文本上（比如128K）预训练模型的成本非常大。
为了让模型能够处理更长的文本，工业界一般先在短文本上预训练模型，然后再利用长文外扩技术让大模型有处理长文本的能力，从而降低训练成本。

## RoPE理论

根据我们在 [长文训练（一）位置编码基础理论](https://zhuanlan.zhihu.com/p/1933621399240569735) 中的分析，以
$$q_m=[x_0, x_1, \ldots, x_{d-1}]$$
为例，计算公式如下：

$$
q_m=
\begin{pmatrix}
\cos m \theta_0 & -\sin m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
\sin m \theta_0 & \cos m \theta_0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m \theta_1 & -\sin m \theta_1 & \cdots & 0 & 0 \\
0 & 0 & \sin m \theta_1 & \cos m \theta_1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\
0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}
\end{pmatrix}
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3 \\
\vdots \\
x_{d-2} \\
x_{d-1}
\end{pmatrix}
\begin{pmatrix}
1
\end{pmatrix}
$$

其中
$$\theta_i = 10000^{-2i/d}$$

让我们回顾一下函数
$$y = 10000^{-x}$$
的图像如下，
$$i$$
越大，则
$$\theta_i$$
越小。**也就是说
$$q_m$$
中维度越高，转过的角度越少**，而且随着token位置$$m$$的增加，转过的角度也会增加。

![img](https://github.com/monster119120/Industrial_LLM_tutorial/raw/main/2_training/algo/long_context/theta_formula.png)


因此我们得出两个趋势性的结论：
1. **当dim维度$$i$$固定时，随着token位置$$m$$的增加，转过的角度会增加**。
2. **当token位置$$m$$固定时，随着dim维度$$i$$的减少，转过的角度会增加**。

TODO