# FlashAttention v2: 极致压榨GPU性能的工程艺术

- FlashAttention 1/2/3
    - [五张图片看懂Flash Attention v1（一）](https://zhuanlan.zhihu.com/p/1936750158621676144)
    - [Flash Attention v2（一）](https://zhuanlan.zhihu.com/p/1936809531221972067)
    - [Flash Attention v3（一）](https://zhuanlan.zhihu.com/p/1936809729683861528)

FlashAttention v1 提出了革命性的算法，而 v2 的目标是**将这个算法在实际硬件（GPU）上运行得尽可能快**。它是一次彻头彻尾的工程优化，其改进可以概括为两大方面：

## 1. 更优的并行计算策略 (Work Partitioning)

GPU 的算力来自于成千上万个并行工作的线程。如何给这些线程分配任务，直接决定了最终效率。

*   **v1 的做法**:
    *   将查询 `Q` 的序列进行分块。一个**线程块 (Thread Block)** 负责处理 `Q` 的一个块。
    *   在这个线程块内部，所有的**线程束 (Warp)** 会**合作**处理一个 `K/V` 的块，完成计算后，再一起去处理下一个 `K/V` 的块。
    *   **问题**: 这种合作模式会导致线程束之间需要同步，如果某个 `K/V` 块的任务分配不均，一些线程束就会**等待**，导致GPU的计算单元闲置，这被称为“低占用率 (low occupancy)”。

*   **v2 的改进 (核心)**:
    *   同样是将 `Q` 序列分块，分配给不同的线程块。
    *   但在线程块内部，v2 做了精妙的调整：它**将 `K/V` 的序列长度也进行了切分**，分配给不同的线程束。
    *   **通俗比喻**: 想象一个大团队（线程块）要组装一批产品（处理一个Q块）。
        *   **v1 的方式**：所有人先一起去1号仓库（第1个K/V块）领零件，组装完；再一起去2号仓库（第2个K/V块）领零件，组装... 团队行动，步调必须一致。
        *   **v2 的方式**：队长直接把1号、2号、3号...仓库的零件（所有K/V块）提前分配给团队里的各个小组（线程束）。每个小组独立负责自己的零件，互不干扰，最后再汇总成果。这样，小组之间无需等待，整个团队的工作效率大大提升。
    *   **效果**: 这种改进减少了线程间的同步和等待，显著提高了GPU的并行处理效率和计算单元的“占用率”。

## 2. 减少非矩阵乘法 (Non-GEMM) 的开销

Attention 计算不仅有 `Q * K^T` 和 `softmax * V` 这样的矩阵乘法 (GEMM)，还有大量的逐元素操作，比如：重新缩放 `O_last`、更新分母 `l`、与 `softmax` 相关的指数和求和运算等。这些操作虽然计算量不大，但频繁的读写会占用宝贵的显存带宽。

*   **v1 的做法**: 按部就班地执行：计算 -> 存储中间结果 -> 读取中间结果 -> 计算下一步...
*   **v2 的改进**:
    *   **重新排布计算和访存指令**。v2 的开发者通过对GPU底层架构的深刻理解，精心设计了指令的执行顺序。比如，在当前块的计算还在进行时，就提前发出指令去预加载下一块所需要的 `K` 和 `V` 数据。
    *   这种技术类似于 **“软件流水线 (Software Pipelining)”**，让计算单元在处理数据A的同时，内存控制器已经在准备数据B，从而将内存访问的延迟隐藏在计算的过程中。
    *   **效果**: 进一步减少了GPU等待数据的时间，使得整个计算流程更加平滑、高效。

**v2 总结**: 通过**更精细的线程任务划分**和**巧妙的指令调度**，FlashAttention v2 让GPU的计算单元和显存带宽都得到了更充分的利用，实现了在相同算法思想下近乎翻倍的性能提升。
